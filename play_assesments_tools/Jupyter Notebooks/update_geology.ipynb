{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config_loader import get_config\n",
    "import AnalyticsAndDBScripts.geo_functions as geo\n",
    "import AnalyticsAndDBScripts.sql_connect as sql\n",
    "import AnalyticsAndDBScripts.sql_schemas as schema\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import logging\n",
    "from sqlalchemy import BigInteger, text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "from shapely import wkt\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs\n",
    "sql_creds_dict = get_config('credentials', 'sql1_sa')\n",
    "grid_config = get_config('geology')\n",
    "\n",
    "# Add db_name to sql_creds_dict\n",
    "sql_creds_dict['db_name'] = 'Analytics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_list = [path['path'] for path in grid_config if 'path' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add grid files to a dictionary of arrays\n",
    "for grid in grid_config:\n",
    "    with open(grid['path'], 'r') as f:\n",
    "        content = f.read()\n",
    "        array = np.loadtxt(io.StringIO(content), delimiter=',')\n",
    "        grid['array'] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid data\n",
    "HEAT_MAPS = False\n",
    "if HEAT_MAPS:\n",
    "    for grid in grid_config:\n",
    "        geo.plot_heatmap_and_histogram(grid['array'], grid['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_statement(config):\n",
    "    interval = config['interval']\n",
    "\n",
    "    # Check if interval is a list\n",
    "    if type(interval) is list:\n",
    "        interval = \"', '\".join([str(i) for i in interval])\n",
    "    sql = f'''\n",
    "    SELECT      W.WellID, W.CurrentCompletionID, C.Interval, C.DataSource, W.Geometry.STAsText() AS Geometry\n",
    "    FROM        dbo.WELL_HEADER W\n",
    "    INNER JOIN  dbo.COMPLETION_HEADER C\n",
    "    ON          W.CurrentCompletionID = C.CompletionID\n",
    "    WHERE       C.Interval IN ('{interval}')\n",
    "    AND         C.DataSource = 'ENVERUS'\n",
    "    '''\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dfs = []\n",
    "\n",
    "for grid in grid_config:\n",
    "    # Execute query and store results in a dataframe\n",
    "    sql_engine = sql.sql_connect(\n",
    "        username=sql_creds_dict['username'], \n",
    "        password=sql_creds_dict['password'], \n",
    "        db_name=sql_creds_dict['db_name'], \n",
    "        server_name=sql_creds_dict['servername'], \n",
    "        port=sql_creds_dict['port']\n",
    "    )\n",
    "    try:\n",
    "        well_df = pd.read_sql(sql_statement(grid), sql_engine).drop_duplicates()\n",
    "    finally:\n",
    "        sql_engine.dispose()\n",
    "\n",
    "    # Convert geometry to a shapely object\n",
    "    well_df['Geometry'] = well_df['Geometry'].apply(wkt.loads)\n",
    "    well_gdf = gpd.GeoDataFrame(well_df, geometry='Geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "    # Filter dataframe to only include valid geometries\n",
    "    valid_geometries = well_gdf['Geometry'].notnull() & ~well_gdf['Geometry'].is_empty\n",
    "    well_gdf = well_gdf[valid_geometries]\n",
    "    df_out = geo.sample_xyz(\n",
    "        df=well_gdf, \n",
    "        file_name=grid['name'], \n",
    "        arr=grid['array'], \n",
    "        epsg=grid['epsg'], \n",
    "        id_col='WellID', \n",
    "        geo_col='Geometry', \n",
    "        sample_method='linear', \n",
    "        input_type=grid['type']\n",
    "    )\n",
    "    df_out.loc[:, 'destination_column'] = grid['destination_column']\n",
    "    sampled_dfs.append(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "df = pd.concat(sampled_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df[df.duplicated(subset=['WellID', 'file_name', 'destination_column'], keep=False)]  \n",
    "\n",
    "# Handle duplicates\n",
    "if not duplicates.empty:\n",
    "    print(\"Duplicates found in the DataFrame based on 'WellID' and 'destination_column'.\")\n",
    "    print(duplicates)  # Optionally, print the duplicates for debugging\n",
    "    raise ValueError(\"Execution stopped due to duplicate entries.\")\n",
    "\n",
    "# Continue with other operations if no duplicates are found\n",
    "print(\"No duplicates found. Continuing with further operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot df where WellID is the index, the columns are the destination columns, and the values are the sampled values\n",
    "df = df.pivot_table(index='WellID', columns='destination_column', values='sampled_z', aggfunc='sum').reset_index()\n",
    "\n",
    "# Add a DataSource column\n",
    "df['DataSource'] = 'ENVERUS'\n",
    "\n",
    "# Convert NaN to None for proper database insertion\n",
    "df = df.where(pd.notnull(df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare staging table schema\n",
    "staging_tbl_name = 'GEO_STAGE'\n",
    "temp_schema = schema.dataframe_to_table_schema(df, staging_tbl_name, {'WellID': BigInteger})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load well_df into dbo.FORECAST_STAGE table in SQL Server\n",
    "sql.load_data_to_sql(df, sql_creds_dict, temp_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the update query\n",
    "query = sql.generate_update_query(\n",
    "    df=df,\n",
    "    dest_table_name='dbo.COMPLETION_HEADER',\n",
    "    source_table_name=f'dbo.{staging_tbl_name}',\n",
    "    join_columns=['WellID', 'DataSource']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    # Connect to the database\n",
    "    sql_engine = sql.sql_connect(\n",
    "        username=sql_creds_dict['username'], \n",
    "        password=sql_creds_dict['password'], \n",
    "        db_name=sql_creds_dict['db_name'], \n",
    "        server_name=sql_creds_dict['servername'], \n",
    "        port=sql_creds_dict['port']\n",
    "    )\n",
    "\n",
    "    # Execute update and drop operations\n",
    "    with sql_engine.connect() as connection:\n",
    "        with connection.begin():\n",
    "            query_text = text(query)\n",
    "            connection.execute(query_text)\n",
    "            logging.info(\"Update successfully executed.\")    \n",
    "            drop_query = text(f\"DROP TABLE dbo.{staging_tbl_name}\")\n",
    "            connection.execute(drop_query)\n",
    "            logging.info(f\"Staging table dbo.{staging_tbl_name} dropped successfully.\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Dispose of the engine explicitly in case of pooled connections\n",
    "    if 'sql_engine' in locals():\n",
    "        sql_engine.dispose()\n",
    "        logging.info(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
