{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import LocalCluster, Client\n",
    "from config.config_loader import get_config\n",
    "import AnalyticsAndDBScripts.sql_connect as sql\n",
    "import AnalyticsAndDBScripts.sql_schemas as schema\n",
    "import AnalyticsAndDBScripts.prod_fcst_functions as fcst\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PYTENSOR_FLAGS\"] = \"floatX=float64,optimizer_excluding=constant_folding\"\n",
    "\n",
    "# Initialize LocalCluster and Client\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials for SQL server\n",
    "sql_creds_dict = get_config('credentials', 'sql1_sa')\n",
    "\n",
    "# Load parameters for the script\n",
    "params_list = get_config('decline_curve')\n",
    "\n",
    "# bifurcate the parameters\n",
    "arps_params = next((item for item in params_list if item['name'] == 'arps_parameters'), None)\n",
    "bourdet_params = next((item for item in params_list if item['name'] == 'bourdet_outliers'), None)\n",
    "changepoint_params = next((item for item in params_list if item['name'] == 'detect_changepoints'), None)\n",
    "b_estimate_params = next((item for item in params_list if item['name'] == 'estimate_b'), None)\n",
    "smoothing_params = next((item for item in params_list if item['name'] == 'smoothing'), None)\n",
    "method_params = next((item for item in params_list if item['name'] == 'method'), None)\n",
    "segment_params = next((item for item in params_list if item['name'] == 'fit_segment'), None)\n",
    "\n",
    "# Create distinct dictionaries for each database\n",
    "sql_aries_creds_dict = sql_creds_dict.copy()\n",
    "sql_aries_creds_dict['db_name'] = 'Analytics_Aries'\n",
    "sql_creds_dict['db_name'] = 'Analytics'\n",
    "\n",
    "# Define parameters\n",
    "value_col = 'Value'\n",
    "fit_segment = changepoint_params['fit_segment']\n",
    "fit_method = 'monte_carlo' # method_params['setting']\n",
    "trials = method_params['trials']\n",
    "fit_months = method_params['fit_months']\n",
    "\n",
    "# Load parameters from config file\n",
    "def_dict = arps_params['terminal_decline']\n",
    "dei_dict1 = arps_params['initial_decline']\n",
    "min_q_dict = arps_params['abandonment_rate']\n",
    "default_b_dict = arps_params['b_factor']\n",
    "\n",
    "# Define columns for output dataframe\n",
    "param_df_cols = [\n",
    "    'WellID', 'Measure', 'fit_months', 'fit_type', 'fit_segment', 'StartDate', \n",
    "    'StartMonth', 'Q_guess', 'Q3', 'Dei', 'b_factor', 'R_squared', 'RMSE', 'MAE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sql query to get wells that need to be forecasted\n",
    "def create_statement_wells(batch_size=500, offset=0):\n",
    "    return f'''\n",
    "    SELECT\t\tF.WellID, F.Measure\n",
    "    FROM\t\tdbo.vw_FORECAST F\n",
    "    INNER JOIN  dbo.WELL_HEADER W\n",
    "    ON          F.WellID = W.WellID\n",
    "    WHERE\t\tF.CumulativeProduction > 0\n",
    "    AND\t\t\tF.LastProdDate > DATEADD(month, -12, GETDATE())\n",
    "    AND\t\t\tW.Trajectory = 'HORIZONTAL'\n",
    "    AND         W.Basin = 'POWDER RIVER'\n",
    "    AND         W.FitGroup = 'NIOBRARA'\n",
    "    ORDER BY\tF.WellID, F.PHASE_INT\n",
    "    OFFSET {offset} ROWS FETCH NEXT {batch_size} ROWS ONLY\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get production data from SQL Server\n",
    "def create_statement(id, measure, cadence='MONTHLY', fit_months=60):\n",
    "    divisor_dict = {'DAILY': 1.0, 'MONTHLY': 30.42}\n",
    "    return f'''\n",
    "    SELECT      WellID, Measure, Date, \n",
    "                Value / COALESCE(NULLIF(ProducingDays, 0), {divisor_dict[cadence]}) AS Value\n",
    "    FROM        dbo.PRODUCTION\n",
    "    WHERE       Cadence = '{cadence}'\n",
    "    AND         WellID = {id}\n",
    "    AND         Measure = '{measure}'\n",
    "    AND         Value > 0\n",
    "    AND         Value IS NOT NULL\n",
    "    AND\t\t\tDate >= DATEADD(month, -{fit_months}, GETDATE())\n",
    "    ORDER BY    WellID, Date\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute query and store results in a dataframe\n",
    "def load_data(creds, statement):\n",
    "    engine = sql.sql_connect(\n",
    "        username=creds['username'], \n",
    "        password=creds['password'], \n",
    "        db_name=creds['db_name'], \n",
    "        server_name=creds['servername'], \n",
    "        port=creds['port']\n",
    "    )\n",
    "    try:\n",
    "        df = pd.read_sql(statement, engine)\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle OperationalError retries when loading data from SQL Server\n",
    "def load_data_with_retry(creds, statement, retries=5, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return load_data(creds, statement)\n",
    "        except OperationalError as e:\n",
    "            print(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add producing months to prod_df\n",
    "def calc_months_producing(group):\n",
    "    min_date = group['Date'].min()\n",
    "    group['MonthsProducing'] = group['Date'].map(lambda x: fcst.MonthDiff(min_date, x))\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from production data\n",
    "def apply_bourdet_outliers(group, date_col, value_col):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        x = group[date_col].values\n",
    "        y = group[value_col].values\n",
    "        y_new, x_new = fcst.bourdet_outliers(y, x, L=bourdet_params['smoothing_factor'], xlog=False, ylog=True, z_threshold=bourdet_params['z_threshold'], min_array_size=bourdet_params['min_array_size'])\n",
    "        group = group[group[date_col].isin(x_new)]\n",
    "        group[value_col] = y_new\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create b_dict from b_factor_df\n",
    "def create_b_dict(b_low, b_avg, b_high, min_b=0.5, max_b=1.4):\n",
    "    # Handle nulls and enforce boundaries for b_low and b_high\n",
    "    b_low = max(b_low if pd.notnull(b_low) else min_b, min_b)\n",
    "    b_high = min(max(b_high if pd.notnull(b_high) else max_b, b_low * 1.1), max_b)\n",
    "    \n",
    "    # Ensure b_avg is between b_low and b_high, and adjust if it's null or out of bounds\n",
    "    if pd.isnull(b_avg) or b_avg < b_low or b_avg > b_high:\n",
    "        b_avg = (b_low + b_high) / 2  # Midpoint if b_avg is not usable\n",
    "    \n",
    "    # Prepare the final dictionary with rounded values to maintain relative differences\n",
    "    return {\n",
    "        'min': round(b_low, 4),\n",
    "        'guess': round(b_avg, 4),\n",
    "        'max': round(b_high, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative function that leverages Markov Chain Monte Carlo (MCMC) sampling for parameter estimation\n",
    "def fit_arps_curve(property_id, phase, b_dict, dei_dict, def_dict, min_q_dict, prod_df_cleaned, value_col, method='curve_fit', trials=1000, fit_segment='all', smoothing_factor=smoothing_params['factor']):\n",
    "    # Function to add the terminal decline rate to the dei_dict\n",
    "    def dict_coalesce(dei_dict, def_dict):\n",
    "        return dei_dict.get('min', def_dict[phase])\n",
    "\n",
    "    # Filter the dataframe to only include the rows for the property_id and phase being analyzed and filter out any rows with 0 or NaN values\n",
    "    df = prod_df_cleaned[\n",
    "        (prod_df_cleaned['WellID'] == property_id) & \n",
    "        (prod_df_cleaned[value_col] > 0) &\n",
    "        (prod_df_cleaned['Measure'] == phase)\n",
    "    ].sort_values(by='Date')\n",
    "\n",
    "    # Identify the fit group for the property_id\n",
    "    df['month_int'] = df['Date'].rank(method='dense', ascending=True)\n",
    "    min_length = 12  # Minimum length of production data desired for fitting\n",
    "\n",
    "    # First, check if the entire DataFrame meets the minimum length requirement\n",
    "    if len(df) <= min_length:\n",
    "        df_selected = df\n",
    "    else:\n",
    "        unique_segments = sorted(df['segment'].unique())\n",
    "        df_selected = pd.DataFrame()  # Initialize an empty DataFrame for the selected data\n",
    "        \n",
    "        if fit_segment == 'first':\n",
    "            segment_index = 0\n",
    "            df_selected = df[df['segment'] == unique_segments[segment_index]]\n",
    "            \n",
    "            # Add data from the next segment until the minimum length is reached\n",
    "            while len(df_selected) < min_length and segment_index + 1 < len(unique_segments):\n",
    "                segment_index += 1\n",
    "                next_segment_df = df[df['segment'] == unique_segments[segment_index]]\n",
    "                df_selected = pd.concat([df_selected, next_segment_df])\n",
    "            \n",
    "        elif fit_segment == 'last':\n",
    "            segment_index = len(unique_segments) - 1\n",
    "            df_selected = df[df['segment'] == unique_segments[segment_index]]\n",
    "            \n",
    "            # Add data from the previous segment until the minimum length is reached\n",
    "            while len(df_selected) < min_length and segment_index - 1 >= 0:\n",
    "                segment_index -= 1\n",
    "                prev_segment_df = df[df['segment'] == unique_segments[segment_index]]\n",
    "                df_selected = pd.concat([prev_segment_df, df_selected])\n",
    "        if len(df_selected) < min_length:\n",
    "            df_selected = df\n",
    "\n",
    "    # Create df and remove the first row from the dataframe for noise reduction\n",
    "    df = df_selected.reset_index(drop=True).iloc[1:]\n",
    "\n",
    "    # Prepare the data for fitting\n",
    "    arr_length = len(df)\n",
    "    t_act = df['Date'].rank(method='min', ascending=True).to_numpy()\n",
    "    q_act = df[value_col].to_numpy()\n",
    "    start_date = df['Date'].min()\n",
    "    start_month = df['month_int'].min()\n",
    "    Qi_guess = np.max(q_act, initial=0)\n",
    "    Dei_init = dei_dict['guess']\n",
    "    Dei_min = dict_coalesce(dei_dict, def_dict)\n",
    "    Dei_max = dei_dict['max']\n",
    "    b_guess = b_dict['guess']\n",
    "\n",
    "    # Ensure bounds are valid\n",
    "    b_min = min(b_dict['min'], b_dict['max'])\n",
    "    b_max = max(b_dict['min'], b_dict['max'])\n",
    "\n",
    "    def auto_fit1(method=method):\n",
    "        bounds = ((Qi_guess*0.9, Dei_min, b_min), (Qi_guess, Dei_max, b_max))\n",
    "        initial_guess = [Qi_guess, Dei_init, b_guess]\n",
    "        config_optimize_qi_dei_b = {\n",
    "            'optimize': ['Qi', 'Dei', 'b'],\n",
    "            'fixed': {'Def': def_dict[phase]}\n",
    "        }\n",
    "        optimized_params = fcst.perform_curve_fit(t_act, q_act, initial_guess, bounds, config_optimize_qi_dei_b, method=method, trials=trials)\n",
    "        qi_fit, Dei_fit, b_fit = optimized_params\n",
    "        # Fitting the curve\n",
    "        q_pred = fcst.varps_decline(1, 1, qi_fit, Dei_fit, def_dict[phase], b_fit, t_act, 0, 0)[3]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            r_squared, rmse, mae = fcst.calc_goodness_of_fit(q_act, q_pred)\n",
    "\n",
    "        return [\n",
    "            property_id, phase, arr_length, 'auto_fit_1', fit_segment, start_date, start_month, \n",
    "            Qi_guess, qi_fit, Dei_fit, b_fit, r_squared, rmse, mae\n",
    "        ]\n",
    "    \n",
    "    def auto_fit2(method=method):\n",
    "        initial_guess = [Dei_init]\n",
    "        bounds = ((Dei_min, Dei_max))\n",
    "        config_optimize_dei = {\n",
    "            'optimize': ['Dei'],\n",
    "            'fixed': {'Qi': Qi_guess, 'b': b_guess, 'Def': def_dict[phase]}\n",
    "        }\n",
    "        optimized_params = fcst.perform_curve_fit(t_act, q_act, initial_guess, bounds, config_optimize_dei, method=method, trials=trials)\n",
    "        Dei_fit = optimized_params[0]\n",
    "        # Fitting the curve\n",
    "        q_pred = fcst.varps_decline(1, 1, Qi_guess, Dei_fit, def_dict[phase], b_dict['guess'], t_act, 0, 0)[3]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            r_squared, rmse, mae = fcst.calc_goodness_of_fit(q_act, q_pred)\n",
    "\n",
    "        return [\n",
    "            property_id, phase, arr_length, 'auto_fit_2', fit_segment, start_date, start_month, \n",
    "            Qi_guess, Qi_guess, Dei_fit, b_guess, r_squared, rmse, mae\n",
    "        ]\n",
    "    \n",
    "    def auto_fit3():      \n",
    "        return [\n",
    "            property_id, phase, arr_length, 'auto_fit_3', fit_segment, start_date, start_month, \n",
    "            Qi_guess, Qi_guess, max(Dei_init, def_dict[phase]), b_guess, np.nan, np.nan, np.nan\n",
    "        ]\n",
    "    \n",
    "    # Case to handle forecasts with less than 3 months of production\n",
    "    if (Qi_guess < min_q_dict[phase]) | (arr_length < 3.0):\n",
    "        result = auto_fit3()\n",
    "    # Case to handle forecasts with more than 2 months and less than 7 months of production\n",
    "    elif arr_length < 7.0:\n",
    "        try:\n",
    "            result = auto_fit2()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed auto_fit2 with error {e}, falling back to auto_fit3\")\n",
    "            result = auto_fit3()\n",
    "    else:\n",
    "        # Apply 3-month rolling average to q_act for smoothing smoothing_factor times\n",
    "        q_act_series = pd.Series(q_act)\n",
    "        \n",
    "        if smoothing_factor > 0:\n",
    "            for i in range(smoothing_factor):\n",
    "                q_act_series = q_act_series.rolling(window=3, min_periods=1).mean()\n",
    "        \n",
    "        q_act = q_act_series.to_numpy()\n",
    "        Qi_guess = np.max(q_act, initial=0)\n",
    "        try:\n",
    "            result = auto_fit1()\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                print(f\"Failed auto_fit1 with error {e1}, trying auto_fit2\")\n",
    "                result = auto_fit2()\n",
    "            except Exception as e2:\n",
    "                print(f\"Failed auto_fit2 with error {e2}, falling back to auto_fit3\")\n",
    "                result = auto_fit3()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process production data\n",
    "def auto_forecast(wellid, measure, sql_creds_dict, value_col, bourdet_params, changepoint_params, b_estimate_params, dei_dict1, default_b_dict, method, smoothing_factor):\n",
    "    # Load production data\n",
    "    prod_df = load_data_with_retry(sql_creds_dict, create_statement(wellid, measure))\n",
    "\n",
    "    # Check if prod_df is empty\n",
    "    if prod_df.empty:\n",
    "        return [wellid, measure, 0, 'no_data', np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "    \n",
    "    # Add MonthsProducing column\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        prod_df = prod_df.groupby(['WellID', 'Measure']).apply(calc_months_producing)\n",
    "        prod_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Apply Bourdet outliers\n",
    "    BOURDET_OUTLIERS = bourdet_params['setting']\n",
    "    if BOURDET_OUTLIERS:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            grouped = prod_df.groupby(['WellID', 'Measure'])\n",
    "            prod_df_cleaned = grouped.apply(apply_bourdet_outliers, 'MonthsProducing', value_col)\n",
    "            prod_df_cleaned.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        prod_df_cleaned = prod_df.copy()\n",
    "\n",
    "    # Segment the time series data using changepoint detection\n",
    "    DETECT_CHANGEPOINTS = changepoint_params['setting']\n",
    "    if DETECT_CHANGEPOINTS:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            cp_penalty = changepoint_params['penalty']\n",
    "            prod_df_cleaned = prod_df_cleaned.groupby(['WellID']).apply(fcst.detect_changepoints, 'WellID', value_col, 'Date', cp_penalty)\n",
    "            prod_df_cleaned.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        prod_df_cleaned['segment'] = 0\n",
    "\n",
    "    # Estimate b factor\n",
    "    ESTIMATE_B_FACTOR = b_estimate_params['setting']\n",
    "    if ESTIMATE_B_FACTOR:\n",
    "        results = fcst.b_factor_diagnostics(prod_df_cleaned, value_col, 'MonthsProducing')\n",
    "        b_dict = create_b_dict(results['b_low'], results['b_avg'], results['b_high'])\n",
    "    else:\n",
    "        b_dict = default_b_dict[measure]\n",
    "\n",
    "    # Fit Arps forecast to production data\n",
    "    result = fit_arps_curve(wellid, measure, b_dict, dei_dict1, def_dict, min_q_dict, prod_df_cleaned, value_col, method, trials, fit_segment, smoothing_factor)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the auto_forecast function to each row in the dataframe\n",
    "def auto_forecast_partition(df, sql_creds_dict, value_col, bourdet_params, changepoint_params, b_estimate_params, dei_dict1, default_b_dict, method, smoothing_factor): \n",
    "    # Apply auto_forecast to each row in the dataframe\n",
    "    results = df.apply(\n",
    "        lambda row: pd.Series(auto_forecast(\n",
    "            row['WellID'],\n",
    "            row['Measure'],\n",
    "            sql_creds_dict,\n",
    "            value_col,\n",
    "            bourdet_params,\n",
    "            changepoint_params,\n",
    "            b_estimate_params,\n",
    "            dei_dict1,\n",
    "            default_b_dict,\n",
    "            method,\n",
    "            smoothing_factor\n",
    "        )).tolist(), axis=1\n",
    "    )\n",
    "    return pd.DataFrame(results.values.tolist(), index=df.index, columns=param_df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and offset to query wells that need to be forecasted\n",
    "batch_size = 100\n",
    "offset = 0\n",
    "param_df_list = []\n",
    "\n",
    "while True:\n",
    "    # Load data into fcst_df and convert to dask dataframe\n",
    "    fcst_df = load_data(sql_creds_dict, create_statement_wells(batch_size, offset))\n",
    "    if fcst_df.empty:\n",
    "        break\n",
    "    fcst_ddf = dd.from_pandas(fcst_df, npartitions=5)\n",
    "\n",
    "    # Define the meta data explicitly\n",
    "    meta_df = pd.DataFrame(columns=param_df_cols)\n",
    "\n",
    "    param_ddf = fcst_ddf.map_partitions(\n",
    "        auto_forecast_partition,\n",
    "        sql_creds_dict,\n",
    "        value_col,\n",
    "        bourdet_params,\n",
    "        changepoint_params,\n",
    "        b_estimate_params,\n",
    "        dei_dict1,\n",
    "        default_b_dict,\n",
    "        fit_method,\n",
    "        smoothing_params['factor'],\n",
    "        meta=meta_df\n",
    "    )\n",
    "\n",
    "    # Compute the Dask dataframe and store the results in a Pandas dataframe\n",
    "    param_df = param_ddf.compute()\n",
    "    param_df_list.append(param_df)\n",
    "\n",
    "    # Increment offset\n",
    "    offset += batch_size\n",
    "\n",
    "param_df = pd.concat(param_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dataframes for loading to SQL Server\n",
    "units_dict = {'OIL': 'BBL', 'GAS': 'MCF', 'WATER': 'BBL'}\n",
    "param_df['Units'] = param_df['Measure'].map(units_dict)\n",
    "param_df['Def'] = param_df['Measure'].map(def_dict)\n",
    "param_df['Qabn'] = param_df['Measure'].map(min_q_dict)\n",
    "param_df['Q1'] = None\n",
    "param_df['Q2'] = None\n",
    "param_df['t1'] = None\n",
    "param_df['t2'] = None\n",
    "param_df['DateCreated'] = pd.to_datetime('today')\n",
    "\n",
    "# Rename some columns\n",
    "param_df.rename(columns={'fit_type': 'Analyst'}, inplace=True)\n",
    "\n",
    "# Select and reorder columns\n",
    "cols = ['WellID', 'Measure', 'Units', 'StartDate', 'Q1', 'Q2', 'Q3', 'Qabn', 'Dei', 'b_factor', 'Def', 't1', 't2', 'Analyst', 'DateCreated']\n",
    "param_df = param_df[cols]\n",
    "\n",
    "# Convert NaN to None for proper database insertion\n",
    "param_df = param_df.where(pd.notnull(param_df), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load well_df into dbo.FORECAST_STAGE table in SQL Server\n",
    "sql.load_data_to_sql(param_df, sql_creds_dict, schema.forecast_stage)\n",
    "\n",
    "# Move data from dbo.FORECAST_STAGE to dbo.FORECAST and drop dbo.FORECAST_STAGE\n",
    "sql.execute_stored_procedure(sql_creds_dict, 'sp_InsertFromStagingToForecast')\n",
    "\n",
    "# Load forecasts into Aries\n",
    "sql.execute_stored_procedure(sql_aries_creds_dict, 'sp_UpdateInsertFromForecastToACECONOMIC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
